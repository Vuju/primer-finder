{
 "cells": [
  {
   "cell_type": "code",
   "id": "439cb9ae5088430e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:30:57.916920Z",
     "start_time": "2025-04-05T19:30:57.754395Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "40bce5c9aa782d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:31:47.117613Z",
     "start_time": "2025-04-05T19:30:58.027507Z"
    }
   },
   "source": "my_result = pd.read_csv(\"../../data/primer-finder-result-0.csv\", sep=\";\")",
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "d8675cddc3961e3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:32:04.211876Z",
     "start_time": "2025-04-05T19:31:47.233158Z"
    }
   },
   "source": [
    "#setup\n",
    "remaining_results = my_result[~my_result['possible_orfs'].str.contains(r'\\[\\]')]\n",
    "solved_results = remaining_results[~remaining_results['possible_orfs'].str.contains(',')]\n",
    "remaining_results = remaining_results[~(remaining_results['Read ID'].isin(solved_results['Read ID']))]\n",
    "\n",
    "solved_results[\"ORF\"] = solved_results.apply(lambda x: x[\"possible_orfs\"][1], axis=1)"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "b33b6e3387719438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:32:22.309513Z",
     "start_time": "2025-04-05T19:32:22.285968Z"
    }
   },
   "source": [
    "from pyhmmer.easel import MSAFile\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pyhmmer\n",
    "from src.orf_finder import process_ambiguous_orf, build_seq_from_pandas_entry\n",
    "\n",
    "\n",
    "def decide_orfs_here(\n",
    "        referenceEntries: pd.DataFrame,\n",
    "        questionableEntries: pd.DataFrame,\n",
    "        translation_table,\n",
    "        e_value = 1000,\n",
    "        pbar=None\n",
    "):\n",
    "    alphabet = pyhmmer.easel.Alphabet.amino()\n",
    "\n",
    "    referenceSequences = np.zeros(shape=len(referenceEntries), dtype=pyhmmer.easel.TextSequence)\n",
    "    referenceSequences.fill(pyhmmer.easel.TextSequence(\"\".encode(),sequence=\"\"))\n",
    "    referenceEntries = referenceEntries.reset_index(drop=True)\n",
    "    for i, row in referenceEntries.iterrows():\n",
    "        referenceSequences[i] = build_seq_from_pandas_entry(row, translation_table=translation_table)\n",
    "\n",
    "    questionableSequences = np.zeros(shape=(len(questionableEntries), 3), dtype=pyhmmer.easel.TextSequence)\n",
    "    questionableSequences.fill(pyhmmer.easel.TextSequence(\"\".encode(),sequence=\"\"))\n",
    "    questionableEntries = questionableEntries.reset_index(drop=True)\n",
    "    for i, row in questionableEntries.iterrows():\n",
    "        questionableSequences[i] = process_ambiguous_orf(row, translation_table=translation_table)\n",
    "\n",
    "    in_file = \"tmp_in.fasta\"\n",
    "    with open(in_file, \"wb\") as f:\n",
    "        for seq in referenceSequences:\n",
    "            #print(seq.sequence)\n",
    "            seq.write(f)\n",
    "    out_file = \"tmp_out.fasta\"\n",
    "    muscle_path = \"/mnt/c/Users/Me/bin/muscle\"\n",
    "    subprocess.run([f\"{muscle_path}\", \"-align\", in_file, \"-output\", out_file],\n",
    "                        stdout=subprocess.PIPE,\n",
    "                        stderr=subprocess.PIPE,\n",
    "                        text=True)\n",
    "\n",
    "    msa = MSAFile(out_file).read()\n",
    "    msa.name = \"tmpMSA\".encode()\n",
    "    background = pyhmmer.plan7.Background(alphabet)\n",
    "    builder = pyhmmer.plan7.Builder(alphabet)\n",
    "    digital_msa = msa.digitize(alphabet=alphabet)\n",
    "    hmm, profile, optimized_profile = builder.build_msa(digital_msa, background)\n",
    "\n",
    "    pipeline = pyhmmer.plan7.Pipeline(alphabet, background)\n",
    "    pipeline.bias_filter = False\n",
    "    pipeline.E = e_value\n",
    "\n",
    "    questionableEntries.loc[:, 'ORF'] = ''\n",
    "    modified_entries = pd.DataFrame(columns=questionableEntries.columns)\n",
    "    total_hits = 0\n",
    "\n",
    "    for query in questionableSequences:\n",
    "\n",
    "        dig_questionable_sequences = [seq.digitize(alphabet=alphabet) for seq in query]\n",
    "        sequenceBlock = pyhmmer.easel.DigitalSequenceBlock(alphabet=alphabet, iterable=dig_questionable_sequences)\n",
    "        top_hits = pipeline.search_hmm(query=hmm, sequences=sequenceBlock)\n",
    "        if len(top_hits.reported) > 0:\n",
    "            total_hits += len(top_hits.reported)\n",
    "            top_hit = top_hits[0]\n",
    "\n",
    "            for hit in top_hits:\n",
    "                if top_hit is not None and top_hit.name.decode() != \"\":\n",
    "                    if hit.evalue < top_hit.evalue:\n",
    "                        top_hit = hit\n",
    "\n",
    "            [read_id, correct_orf] = top_hit.name.decode().split(\"_\")\n",
    "            questionableEntries.loc[questionableEntries['Read ID'] == read_id, 'ORF'] = correct_orf\n",
    "            if len(modified_entries[modified_entries[\"Read ID\"] == read_id]) == 0:\n",
    "                modified_entries = pd.concat([modified_entries, questionableEntries.loc[questionableEntries['Read ID'] == read_id].copy()], ignore_index=True)\n",
    "        ### this is the much slower but more responsive updater (see comment further up)\n",
    "        if pbar is not None:\n",
    "            pbar.update(1)\n",
    "    print(f\"modified entries: {len(modified_entries)} (of {total_hits} hits) and {len(questionableEntries)} original entries\")\n",
    "    return modified_entries"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "853f08c944d93be6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:32:29.516122Z",
     "start_time": "2025-04-05T19:32:22.860583Z"
    }
   },
   "source": [
    "#params\n",
    "threshold = 10\n",
    "upper_threshold = 50\n",
    "\n",
    "debugFrame = pd.DataFrame\n",
    "failed = 0\n",
    "taxonomic_levels = ['Species', 'Genus', 'Family', 'Order', 'Class']\n",
    "\n",
    "while remaining_results.size > 0:\n",
    "    current_entry = remaining_results.iloc[0]\n",
    "\n",
    "    # Try to match at each taxonomic level, from specific to general\n",
    "    for level in taxonomic_levels:\n",
    "        level_value = current_entry[level]\n",
    "        comp_group = solved_results[solved_results[level] == level_value]\n",
    "        group_size = len(comp_group)\n",
    "\n",
    "        if group_size >= threshold:\n",
    "            comp_group = comp_group.sample(min(upper_threshold, group_size))\n",
    "            related_entries = remaining_results[remaining_results[level] == level_value]\n",
    "            solved = decide_orfs_here(comp_group, related_entries,translation_table=5)\n",
    "            solved_results = pd.concat([solved_results, solved], ignore_index=True)\n",
    "            remaining_results = remaining_results[~(remaining_results[level] == level_value)]\n",
    "            break\n",
    "        else:\n",
    "            print(f\"{group_size} entries of {level} {level_value} is too small.\")\n",
    "\n",
    "            # If we've tried all levels and none are big enough\n",
    "            if level == taxonomic_levels[-1]:\n",
    "                print(f\"Removing Family '{current_entry['Family']}' with {len(solved_results[solved_results['Family'] == current_entry['Family']])} members to continue.\")\n",
    "                remaining_results = remaining_results[~(remaining_results['Family'] == current_entry['Family'])]\n",
    "                failed += len(related_entries) if 'related_entries' in locals() else 1\n",
    "\n",
    "    print(\"-------------\")\n",
    "\n",
    "print(f\"A total of {failed} entries were impossible to match.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified entries: 6 (of 6 hits) and 6 original entries\n",
      "-------------\n",
      "modified entries: 7 (of 7 hits) and 7 original entries\n",
      "-------------\n",
      "modified entries: 1 (of 1 hits) and 1 original entries\n",
      "-------------\n",
      "1 entries of Species s:Oedothorax_trilobatus is too small.\n",
      "modified entries: 66 (of 66 hits) and 66 original entries\n",
      "-------------\n",
      "modified entries: 1 (of 1 hits) and 1 original entries\n",
      "-------------\n",
      "modified entries: 43 (of 43 hits) and 44 original entries\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[73]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     20\u001B[39m related_entries = remaining_results[remaining_results[level] == level_value]\n\u001B[32m     21\u001B[39m solved = decide_orfs_here(comp_group, related_entries,translation_table=\u001B[32m5\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m solved_results = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43msolved_results\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msolved\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m remaining_results = remaining_results[~(remaining_results[level] == level_value)]\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/MA/lib/python3.12/site-packages/pandas/core/reshape/concat.py:395\u001B[39m, in \u001B[36mconcat\u001B[39m\u001B[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[39m\n\u001B[32m    380\u001B[39m     copy = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    382\u001B[39m op = _Concatenator(\n\u001B[32m    383\u001B[39m     objs,\n\u001B[32m    384\u001B[39m     axis=axis,\n\u001B[32m   (...)\u001B[39m\u001B[32m    392\u001B[39m     sort=sort,\n\u001B[32m    393\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m395\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/MA/lib/python3.12/site-packages/pandas/core/reshape/concat.py:684\u001B[39m, in \u001B[36m_Concatenator.get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    680\u001B[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001B[32m    682\u001B[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001B[32m--> \u001B[39m\u001B[32m684\u001B[39m new_data = \u001B[43mconcatenate_managers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    685\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmgrs_indexers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnew_axes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconcat_axis\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbm_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\n\u001B[32m    686\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    687\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.copy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_copy_on_write():\n\u001B[32m    688\u001B[39m     new_data._consolidate_inplace()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/MA/lib/python3.12/site-packages/pandas/core/internals/concat.py:177\u001B[39m, in \u001B[36mconcatenate_managers\u001B[39m\u001B[34m(mgrs_indexers, axes, concat_axis, copy)\u001B[39m\n\u001B[32m    167\u001B[39m vals = [ju.block.values \u001B[38;5;28;01mfor\u001B[39;00m ju \u001B[38;5;129;01min\u001B[39;00m join_units]\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m blk.is_extension:\n\u001B[32m    170\u001B[39m     \u001B[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001B[39;00m\n\u001B[32m    171\u001B[39m     \u001B[38;5;66;03m#  we can use np.concatenate, which is more performant\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    175\u001B[39m     \u001B[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001B[39;00m\n\u001B[32m    176\u001B[39m     \u001B[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     values = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_1d_only_ea_dtype(blk.dtype):\n\u001B[32m    179\u001B[39m     \u001B[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001B[39;00m\n\u001B[32m    180\u001B[39m     values = concat_compat(vals, axis=\u001B[32m0\u001B[39m, ea_compat_axis=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3521f86c0acc6a5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
